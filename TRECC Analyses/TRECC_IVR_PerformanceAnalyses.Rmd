---
title: "TRECC IVR Performance Analyses - 2019 Full Study"
author: "Evelyn Yarzebinski, Benjamin Zinszer, Mackenzie Campbell"
output:
   html_document:
     toc: true
     toc_float: true
     theme: united
---

##0. Metadata
```{r prep data, include = F}
#clean up environment
rm(list=ls())
gc(verbose=TRUE)

source("TRECC_dataPrep_fullStudy_HD.R")

knitr::knit_hooks$set(
   error = function(x, options) {
     paste('\n\n<div class="alert alert-danger">',
           gsub('##', '\n', gsub('^##\ Error', '**Error**', x)),
           '</div>', sep = '\n')
   },
   warning = function(x, options) {
     paste('\n\n<div class="alert alert-warning">',
           gsub('##', '\n', gsub('^##\ Warning:', '**Warning**', x)),
           '</div>', sep = '\n')
   },
   message = function(x, options) {
     paste('\n\n<div class="alert alert-info">',
           gsub('##', '\n', x),
           '</div>', sep = '\n')
   }
)
```

###Report Generation
```{r report time, echo = F}
message(paste("report generated: ",Sys.time(),sep=""))

```

###What is the min/max and span of the data?
####Note: Date shown here is the most recent day in the dataset and may be incomplete due to time of data export.
```{r date range, echo = F}
UAS_maxDate = max(UASdata_filter$date)
UAS_minDate = min(UASdata_filter$date)
CDR_maxDate = max(cdrData_userCalls$date)
CDR_minDate = min(cdrData_userCalls$date)
interactions_maxDate = max(interactionsData$date)
interactions_minDate = min(interactionsData$date)
UAS_nDays = UAS_maxDate - UAS_minDate + 1
CDR_nDays = CDR_maxDate - CDR_minDate + 1
interactions_nDays = interactions_maxDate - interactions_minDate + 1
message(paste("user_answer_stats ranges from ", UAS_minDate," to ", UAS_maxDate,". The span is ",UAS_nDays," days.", sep = ""))
message(paste("cdr_ivr01 ranges from ", CDR_minDate, " to ", CDR_maxDate,". The span is ",CDR_nDays," days.", sep = ""))
message(paste("interactions ranges from ", interactions_minDate, " to ", interactions_maxDate,". The span is ",interactions_nDays," days.", sep = ""))

```

```{r saving, include=F}

#UASdata_filter = UASdata_filter %>%
#  select(-c(users.mobile_number, sessions.mobile_number, localPhoneNumber))

#write.csv(UASdata_filter,paste("~/Documents/IvoryCoast/data/UASdata_currentDataThrough_",UAS_maxDate,".csv",sep=""),row.names = FALSE)
#write.xlsx(UASdata_filter,paste("~/Documents/IvoryCoast/data/UASdata_currentDataThrough_",UAS_mostRecentDate,".xlsx",sep=""),row.names = FALSE)
#write.csv(cdrData_filter,paste0("~/Documents/IvoryCoast/data/cdrData_currentDataThrough_",CDR_mostRecentDate,".csv"),row.names = FALSE)
#write.xlsx(cdrData_filter,paste("~/Documents/IvoryCoast/data/cdrData_currentDataThrough_",CDR_mostRecentDate,".xlsx",sep=""),row.names = FALSE)
```

###Explore the data
```{r Ben data exploration, echo = F}
datatable(
  UASdata_filter.tokenid, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

##1. Question Summary
###Exploratory tables
####Originally, these were called "question templates" and then became "VOS" (voice output structure) or even "question frames" - basically, the raw question structure that different tokens are plugged into.

###{.tabset}
####Number of Questions Per Lesson
```{r questions per lesson, echo = F}
questionsPerLesson = UASdata_filter %>%
  group_by(UAS.unit_id, cmsQuestions.question_number, UAS.lesson_id) %>%
  summarize(nQuestionsPerLesson = n_distinct(cmsQuestions.question_text))

questionsPerLesson_V02 = questionsPerLesson %>%
  group_by(UAS.unit_id, cmsQuestions.question_number) %>%
  summarize(nLessons = n_distinct(UAS.lesson_id),
            avgQuestionsPerLesson = round(mean(nQuestionsPerLesson),2),
            lessonsWithLessThan5Q = length(nQuestionsPerLesson[nQuestionsPerLesson<=4]),
            min = min(nQuestionsPerLesson),
            max = max(nQuestionsPerLesson))

datatable(
  questionsPerLesson_V02, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))

```

####Unique Question Types and Performance
```{r VOS performance, echo = F}
questionPerformance = UASdata_filter %>%
  group_by(UAS.unit_id,cmsQuestions.question_number) %>%
  summarize(nStudents = n_distinct(studentStudyId),
            totalAttempts = sum(UAS.number_of_attempts),
          avgCorrectnessOfQuestionAttempts = round(mean(UAS.correct),2))

datatable(
  questionPerformance, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

###Question List
####The full list of all unique combinations of question_type and token A/B/C combinations that students have attempted
####{.tabset}
#####All Questions
```{r question list, echo= F}
questionList = UASdata_filter %>%
  group_by(UAS.unit_id, cmsQuestions.question_number,cmsQuestions.difficulty_level_trial,cmsQuestions.question_text) %>%
  summarize(nStudents = n_distinct(studentStudyId),
            nAttempts = sum(inMichelsList),
            avgCorrectness = round(mean(UAS.correct),2))

datatable(
  questionList, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

#####Questions Repeated Multiple Days
```{r}
questionList_Repeated = UASdata_filter %>%
  group_by(UAS.unit_id, studentStudyId,cmsQuestions.question_number,cmsQuestions.difficulty_level_trial,cmsQuestions.question_text) %>%
  summarize(nDays = n_distinct(date),
            nAttempts = sum(inMichelsList),
            avgCorrectness = round(mean(UAS.correct),2)) %>%
  filter(nDays > 1)

datatable(
  questionList_Repeated, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

###Summary of Question Distribution
```{r question summary, echo = F}
message(paste("Currently across all units:",n_distinct(questionList$cmsQuestions.question_text),"unique questions."))
message(paste("There are",nrow(questionList),"rows in the Question List above."))

```

<!-- ###Does a single question have multiple difficulty values? -->
<!-- ```{r} -->
<!-- #output the questions with multiple difficulty levels. should this be? -->


<!-- ``` -->

###Do any trials have chance accuracy even after 3 attempts?
####Two-option questions (cannot be re-attempted) show low accuracy rates on the first and only attempt.
```{r, echo = F}
datatable(
  UASdata_filter.trialid[UASdata_filter.trialid$zLastAcc<1,c('trialID','students','presentations','options','LastAcc','zLastAcc')], 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

####Three-option questions improve considerably.
```{r, echo = F}
datatable(
  UASdata_filter.trialid, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

###Comparing student performance across quartile splits on given unit/question types
####Ideally, there should be a discernible difference between Q1 and Q4 in each graph. If there is not, it may indicate question difficulty, technical difficulty, student unreadiness, etc.
####The plot shows the quartile split of all questions a student receives (x axis) by the average % correct on that quartile of questions (y axis). Each combination of a unit and question in its own plot (each plot's title corresponds to "Unit#_Question#"). From Quartile 1 to Quartile 4, we would hope to see the students performing better.


####{.tabset} 
#####Table
```{r performance first lesson vs current lesson, echo = F}
performanceFirstLessonToCurrent = UASdata_filter %>%
  group_by(studentStudyId, UAS.unit_id, cmsQuestions.question_number) %>%
  mutate(questionAndUnit_concat = paste0(UAS.unit_id, "_",cmsQuestions.question_number),
         questionMax = max(questionNumberPerUnit),
         questionQuartile = questionNumberPerUnit / questionMax,
    #questionQuantile = rank(questionNumberPerUnit)/length(unique(questionNumberPerUnit)),
         questionQuartileGroup = ifelse(questionQuartile < .25, "Q1", 
                                        ifelse(questionQuartile >= .75, "Q2", 
                                               ifelse(questionQuartile >= .5, "Q3", "Q4")))) %>%
  group_by(studentStudyId, questionAndUnit_concat,usersToUnits.currentUnit,questionQuartileGroup) %>%
  summarize(avgCorrectPercent = round(mean(UAS.correct),2),
            sdCorrectPercent = round(sd(UAS.correct),2),
            nQuestions = n_distinct(questionNumberOverall))
  # group_by(questionAndUnit_concat, questionQuantileGroup) %>%
  # summarize(avgCorrectPercent = round(mean(avgCorrectPercent),2),
  #           sdCorrectPercent = round(sd(sdCorrectPercent),2),
  #           avgNQuestions = round(mean(nQuestions),2),
  #           sdNQuestions = round(sd(nQuestions),2),
  #           nStudents = n_distinct(studentStudyId))

datatable(
  performanceFirstLessonToCurrent, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

#####Plot
```{r, echo=F}
 performanceFirstLessonToCurrent %>%
 ggplot(aes(x = questionQuartileGroup, y = avgCorrectPercent)) +
   geom_jitter(alpha = .2, width = .2) +
   #stat_smooth(aes(group = usersToUnits.currentUnit)) +
   facet_wrap(~ questionAndUnit_concat, ncol = 5)
```



##2. Token Summary

###Token Distribution
####difficulty_level_token increases by 1 if certain phonemes don't exist in AttiÃ© and also if the token contains a certain syllable structure.
####difficulty_level_trial increases by 1 for each shared phoneme (in the same position) between tokens.
```{r token distribution, echo = F}
tokensInQuestions = UASdata_filter %>%
  group_by(UAS.unit_id, cmsQuestions.question_number, cmsQuestions.difficulty_level_trial, cmsQuestions.difficulty_level_token) %>%
  summarize(nStudents = n_distinct(studentStudyId),
            totalUniqueQuestions = n_distinct(cmsQuestions.id),
            totalAttempts = sum(inMichelsList),
            uniqueTokenA = n_distinct(cmsQuestions.token_a_id),
            uniqueTokenB = n_distinct(cmsQuestions.token_b_id),
            uniqueTokenC = n_distinct(cmsQuestions.token_c_id))

datatable(
  tokensInQuestions, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```


###Token Breakdown 
#####How to read this table: Starting with row 1, a row can be interpreted as 'token_a_id #20 was experienced by 5 students in 26 attempts of 3 unique questions, with an average correct respose rate of 0.69'.
#####Sorting by avgAttemptCorrectness reveals a number of token_ids that have an average correctness below chance. This may indicate a number of different possibilities: the token is very difficult, the token is not being played so students are guessing because they hear silence, their environment was noisy and so they didn't hear the options, etc.

###{.tabset}

#### Token A List (correct answers)
```{r token list A, echo = F}
tokenDistribution_A = UASdata_filter %>%
  group_by(cmsQuestions.token_a_id, phonetics_auditory_token_a, spelling_visual_token_a) %>%
  summarize(nStudents = n_distinct(studentStudyId),
            uniqueQuestionTypesWithTokenA = n_distinct(cmsQuestions.id),
            countAttempts = sum(inMichelsList),
            avgAttemptCorrectness = round(mean(UAS.correct),2))


datatable(
  tokenDistribution_A, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

####Token B List (distractor)
```{r token list B, echo = F}
tokenDistribution_B = UASdata_filter %>%
  group_by(cmsQuestions.token_b_id, phonetics_auditory_token_b, spelling_visual_token_b) %>%
  summarize(nStudents = n_distinct(studentStudyId),
            uniqueQuestionsTypesWithTokenB = n_distinct(cmsQuestions.id),
            countAttempts = sum(inMichelsList),
            avgAttemptCorrectness = round(mean(UAS.correct),2))

datatable(
  tokenDistribution_B, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
  
```

####Token C List (distractor)
```{r token list C, echo = F}
tokenDistribution_C = UASdata_filter %>%
  group_by(cmsQuestions.token_c_id, phonetics_auditory_token_c, spelling_visual_token_c) %>%
  summarize(nStudents = n_distinct(studentStudyId),
            uniqueQuestionTypesWithTokenC = n_distinct(cmsQuestions.id),
            countAttempts = sum(inMichelsList),
            avgAttemptCorrectness = round(mean(UAS.correct),2))

datatable(
  tokenDistribution_C, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

###Distractor Token Co-Occurrence?
####distractor_token value of "null" occurs on true/false questions. I left that in for comparison to the non-true/false questions.
```{r distractor token co-occurrence, echo = F}

tokenDistribution_distractorBC = UASdata_filter %>%
  ungroup() %>%
  group_by(cmsQuestions.distractor_tokens_V02, cmsQuestions.distractor_tokens_IPA, cmsQuestions.distractor_tokens_spelling) %>%
  summarize(nStudents = n_distinct(studentStudyId),
            uniqueQuestionTypesWithTokenPair = n_distinct(cmsQuestions.id),
            countAttempts = sum(inMichelsList),
            avgAttemptCorrectness = round(mean(UAS.correct),2))

datatable(
  tokenDistribution_distractorBC, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

###Summary of Token Distribution
```{r token summary, echo = F}
message(paste("Currently across all tokens:",nrow(tokenDistribution_A),"unique token As,",nrow(tokenDistribution_B), "unique token Bs, and",nrow(tokenDistribution_C),"unique token Cs have been used in at least one question. There are",nrow(tokenDistribution_distractorBC),"unique combinations of Tokens B and C."))
```

### Which tokens are most frequently presented?
####The histogram indicates the number of times tokens were presented, e.g. the first bar indicates that ~17 unique tokens were produced a total of 1 time to students.
####High Frequency tokens, based on standard deviation of selection rates). Low frequency tokens don't appear in the data since zero-frequency tokens are by default excluded.

####{.tabset}
#####Plot
```{r, echo = F}

UASdata_filter.tokenid %>%
  ggplot(aes(x = presentations)) +
  geom_histogram(binwidth = 1)

#original hist code from Ben
#hist(UASdata_filter.tokenid$presentations)
tweight.mean <- round(mean(UASdata_filter.tokenid$presentations),2)
tweight.sd <- round(sd(UASdata_filter.tokenid$presentations),2)

# Low frequency tokens (code included for completeness, but commented out for now)
# Spoiler, there aren't any (since the zero-frequency tokens don't get included)
#UASdata_filter.tokenid[UASdata_filter.tokenid$presentations < tweight.mean-2*tweight.sd,]
```

#####Table
```{r, echo = F}
# High frequency tokens

datatable(
  UASdata_filter.tokenid[UASdata_filter.tokenid$presentations > tweight.mean+2*tweight.sd,], 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))

```

###Which tokens have low accuracy in first and last attempts?
####Low accuracy on first attempts
####{.tabset}
#####Plot
```{r, echo = F}
UASdata_filter.tokenid %>%
  ggplot(aes(x = FirstAcc)) +
  geom_histogram(binwidth = .1 )
```

#####Table
```{r}
#hist(UASdata_filter.tokenid$FirstAcc)
# Low accuracy first-attempts: 
# When there are at least 5 presentations to average across, and accuracy is equal to or below chance

datatable(
  UASdata_filter.tokenid[UASdata_filter.tokenid$zFirstAcc<=0 & UASdata_filter.tokenid$presentations>=5,], 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

####Low accuracy on last attempts
####{.tabset}
#####Plot
```{r, echo = F}
UASdata_filter.tokenid %>%
  ggplot(aes(x = LastAcc)) +
  geom_histogram(binwidth = .1 )
```

#####Table
```{r}
#hist(UASdata_filter.tokenid$LastAcc)
# Low accuracy last-attempts tokens:
# When there are at least 5 presentations to average across, and accuracy is near or below chance
# Even some greater-than-chance results included, because they should be learnable by 3rd attempt

datatable(
  UASdata_filter.tokenid[UASdata_filter.tokenid$zLastAcc<=1 & UASdata_filter.tokenid$presentations>=5,], 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))

```

###Does token difficulty rating accurately predict first- or last-attempt performance? 
###{.tabset}

####Difficulty (first attempt)

```{r, echo = F}
token_diff <- cor.test(UASdata_filter.tokenid$tokenDifficulty, UASdata_filter.tokenid$zFirstAcc,weights=UASdata_filter.tokenid$presentations)
token_diff
```   
 
####Learnability (last attempt)
```{r, echo = F}
token_lern <- cor.test(UASdata_filter.tokenid$tokenDifficulty, UASdata_filter.tokenid$zLastAcc,weights=UASdata_filter.tokenid$presentations)
token_lern
```

##3. Syllable analysis
###What syllable structures have been used and what have been excluded so far?
####Blank FirstAcc or LastAcc means that the token has not been presented to any students yet.
```{r, echo = F}
# Count the number of total presentations for each syllable structure
# Check the difficulty and learnability of each of the presented syllable structures
#UASdata_filter.syllable <- UASdata_filter.tokenid.ALLtokens %>%
UASdata_filter.syllable <- UASdata_filter.tokenid %>%
  ungroup() %>%
  group_by(token_a_syllable_structure) %>%
  summarise(
    presentations = sum(presentations,na.rm=TRUE),
    FirstAcc = round(mean(FirstAcc,na.rm=TRUE),2),
    LastAcc = round(mean(LastAcc,na.rm=TRUE),2)
  )

datatable(
  UASdata_filter.syllable, 
  extensions = 'FixedColumns',
  options = list(
  scrollX = TRUE,
  scrollCollapse = TRUE
))
```

###Does syllable structure significantly affect the token difficulty?
####These are the syllables that have appeared in questions so far.
####The percent under each syllable structure indicates the percent of the attempts that include that structure.
```{r, echo = F}
UASdata_filter.tokenid$token_a_syllable_structure = as.factor(UASdata_filter.tokenid$token_a_syllable_structure)
UASdata_filter.tokenid$token_a_syllable_structure <- relevel(UASdata_filter.tokenid$token_a_syllable_structure,'CVC')
#unique(UASdata_filter.tokenid$token_a_syllable_structure)
round(prop.table(table(UASdata_filter.tokenid$token_a_syllable_structure)),2)

```

####{.tabset}
#####First attempt
```{r, echo = F}
# Perform ANOVA to determine whether syllable structures significantly differ from each other in first-attempt accuracy
syllable_effect_zF <- lm(zFirstAcc~token_a_syllable_structure,UASdata_filter.tokenid,weight=UASdata_filter.tokenid$presentations)
#summary(syllable_effect_zF)
anova(syllable_effect_zF)
```

#####Last attempt
```{r, echo = F}
# Perform ANOVA to determine whether syllable structures significantly differ from each other in last-attempt accuracy
syllable_effect_zL <- lm(zLastAcc~token_a_syllable_structure,UASdata_filter.tokenid,weight=UASdata_filter.tokenid$presentations)
#summary(syllable_effect_zL)
anova(syllable_effect_zL)
```

<!-- # ```{r} -->
<!-- # summary(lmer(UAS.correct ~ UAS.unit_id + cmsQuestions.difficulty_level_token + cmsQuestions.difficulty_level_trial + (1|studentStudyId), data = UASdata_filter)) -->
<!-- #  -->
<!-- # ``` -->

##4. Token Type Analysis

###Does token type significantly affect the token difficulty?
####List the current token types
```{r, echo = F}
UASdata_filter.tokenid$token_a_type = as.factor(UASdata_filter.tokenid$token_a_type)
round(prop.table(table(UASdata_filter.tokenid$token_a_type)),2)
message("note: 1 = word; 2 = syllable; 3 = phoneme")
```
